# vLLM + Nsight Systems

Simple vLLM model inference on GPU with Nsight Systems profiling.

## Setup
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Run

**activate venv first:**
```bash
source venv/bin/activate
```

**run inference with benchmarking:**
```bash
python3 vllm_demo.py
```
generates: `benchmark_<model>_<timestamp>.json`

**profile with nsight:**
```bash
nsys profile -o profile_facebook_opt-125m --force-overwrite true python3 vllm_demo.py
```
generates: `profile_facebook_opt-125m.nsys-rep`

**view profile:**
```bash
nsys-ui profile_facebook_opt-125m.nsys-rep
```

**compare multiple models:**
```bash
python3 compare_benchmarks.py
```
generates:
- `plots/comparison_table_<timestamp>.png` - comparison table
- `plots/performance_graphs_<timestamp>.png` - 4 performance graphs

## Model Comparison

**to test different models:**
1. edit `model_name` in `vllm_demo.py` (line 11)
2. run the script
3. profile files auto-named by model

**suggested models for GTX 1650 (4GB):**
- `facebook/opt-125m` - 125M params, ~500MB
- `facebook/opt-350m` - 350M params, ~700MB
- `gpt2` - 124M params, ~500MB
- `gpt2-medium` - 355M params, ~700MB
- `distilgpt2` - 82M params, ~350MB

**larger models (may need lower gpu_memory_utilization):**
- `facebook/opt-1.3b` - 1.3B params, ~2.5GB
- `gpt2-large` - 774M params, ~1.5GB

## performance graphs

`compare_benchmarks.py` generates visualizations in `plots/` directory:

**comparison table** - side-by-side metrics for all models

**performance graphs** - 4 charts showing:
1. inference speed (tokens/second) - higher is better
2. inference time (seconds) - lower is better
3. gpu memory utilization (%) - with 80% threshold line
4. load time vs inference time - side-by-side comparison

## Nsight Output

shows nvtx ranges, cuda api calls, gpu kernels, and memory transfers (M2D during load, D2D during inference)

**available nsight profiles:**
- `profile_facebook_opt-125m.nsys-rep` - [output_opt125m.png](output_opt125m.png)
- `profile_facebook_opt-350m.nsys-rep` - [output_opt350m.png](output_opt350m.png)
- `profile_facebook_opt-1.3b.nsys-rep` - [output_opt1.3b.png](output_opt1.3b.png)
- `profile_gpt2-large-774m.nsys-rep` - [output_gpt2-large-774m.png](output_gpt2-large-774m.png)

## What to expect in Nsight Systems

The profile will show:
- **NVTX ranges**: `model_initialization`, `inference`, `output_processing`
- **GPU utilization graph**: Blue graph showing GPU activity
- **CUDA API calls**: Memory allocations, kernel launches
- **CUDA kernels**: Individual GPU operations during inference

**Note:** Uses vLLM 0.6.3 which is compatible with CUDA 12.0

## Benchmark Properties Reference

### model_properties
- `total_parameters` - total number of model parameters (int)
- `num_layers` - number of transformer layers (int)
- `hidden_size` - hidden dimension size (int)
- `vocab_size` - vocabulary size (int)
- `model_size_mb` - GPU memory used by model weights (float, MB)

### inference_metrics
- `model_load_time_seconds` - time to load model into GPU (float, seconds)
- `total_inference_time_seconds` - time for inference execution (float, seconds)
- `tokens_per_second` - throughput rate (float, tokens/s)
- `num_prompts` - number of input prompts (int)
- `total_input_tokens` - total input tokens across all prompts (int)
- `total_output_tokens` - total generated tokens (int)
- `average_tokens_per_prompt` - avg output tokens per prompt (float)

### gpu_metrics
- `gpu_name` - GPU device name (string)
- `gpu_memory_total_mb` - total GPU memory available (float, MB)
- `gpu_memory_allocated_mb` - GPU memory allocated by model (float, MB)
- `gpu_memory_reserved_mb` - GPU memory reserved by PyTorch (float, MB)
- `gpu_memory_utilization_percent` - percentage of GPU memory used (float, %)
- `peak_memory_allocated_mb` - peak GPU memory during inference (float, MB)

### nsight_metrics
- `profile_file` - nsight profile filename (string)
- `profile_name` - profile identifier (string)
- `notes` - profiling notes about memory transfers (string)

### comparison_table (generated by compare_benchmarks.py)
- `Model` - model name with parameter count (string)
- `Params` - total parameters (int)
- `Layers` - number of layers (int)
- `Model Size (MB)` - GPU memory for weights (float)
- `Load Time (s)` - model loading time (float)
- `Inference Time (s)` - inference execution time (float)
- `Tokens/s` - throughput (float)
- `GPU Mem (MB)` - allocated GPU memory (float)
- `GPU Util (%)` - GPU memory utilization (float)
