{
  "_comment": "This is a TEMPLATE showing the benchmark JSON structure. Actual data is generated by running vllm_demo.py",
  "model_name": "facebook/opt-125m",
  "timestamp": "2026-01-29T09:57:00",
  "model_properties": {
    "total_parameters": 125000000,
    "num_layers": 12,
    "hidden_size": 768,
    "vocab_size": 50272,
    "model_size_mb": 238.9
  },
  "inference_metrics": {
    "total_runtime_seconds": 0.0,
    "prefill_time_seconds": 0.0,
    "decode_time_seconds": 0.0,
    "tokens_per_second": 0.0,
    "num_prompts": 2,
    "total_input_tokens": 0,
    "total_output_tokens": 0
  },
  "gpu_metrics": {
    "gpu_name": "NVIDIA GeForce GTX 1650",
    "gpu_memory_total_mb": 0,
    "gpu_memory_allocated_mb": 0,
    "gpu_memory_reserved_mb": 0,
    "gpu_memory_utilization_percent": 0.0,
    "peak_memory_allocated_mb": 0
  },
  "nsight_metrics": {
    "profile_file": "vllm_profile.nsys-rep",
    "notes": "M2D (host-to-device) transfers occur during model loading. D2D (device-to-device) and kernel execution during inference. KV cache usage tracked via GPU block allocation."
  }
}
